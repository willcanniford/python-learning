{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and dimensionality reduction\n",
    "Using PCA is a method of dimensionality reduction that aims at reducing it to only the parts that hold the most information. It's main goal is to reduce the number of features in a dataset while capturing a large amount of the variability contained within the data. \n",
    "\n",
    "\n",
    "## Why it is useful\n",
    "When you have a large dataset, you can offer suffer from the \"curse of dimensionality\". This means that you need to reduce the number of features (or dimensions) before you can effective start building a model. Feature selection, and feature extraction are two ways of approaching dimensionality reduction.  \n",
    "\n",
    "We can use PCA to extract new \"latent features\" (these are features that aren't explicityly in your dataset; think \"performance\" or \"clutch\" that might be produced by a variety of other features) from our dataset that are based on existing features; this allows us to reduce the number of dimensions overall but hold on to the parts that hold the most information and represent the highest variance. This can be done easily with `sklearn`.  \n",
    "\n",
    "Once you have produced and carried out a PCA, you will look at the principal components, and the variability of the original data that has been captured by them; this is the main output from a principal component analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Dimensionality Reduction\n",
    "This isn't always necessary, but there are some problems and data sets that are going to involve a lot of features, hundreds perhaps thousands.  \n",
    "\n",
    "There are two main approaches to dimensionality reduction: \n",
    "1. Feature Selection\n",
    "2. Feature Extraction\n",
    "\n",
    "### Feature Selection\n",
    "This is finding a subset of the original features and determining which of those are most useful. There are a few methods for selecting features: \n",
    "* __Filter methods__ use a ranking or sorting alogirthm to filter out those that are deemed less useful. They focus on discerning some inherent correlations among the feature data in unsupervised learning, or on correlations with the output variable in supervised settings. These filter methods are usually applied as a preprocessing step. Common tools include Pearson's Correlation, Linear Discriminant Analysis (LDA) and Analysis of Variance (ANOVA). \n",
    "* __Wrapper methods__ work through selecting features by directly testing their impact on the performance of a model. The general idea of this method is to call the alogirthm while building models using different subsets of features, and measuring the performance of each of these models. Cross-validation is used across these multiple tests, and then the features that provide the best results are selected; this is obviously computationally intense. Common examples are Forward Search, Backward Search and Recursive Feature Elimination. \n",
    "\n",
    "`sklearn` has a feature selection module that offers a variety of methods to improve model accuracy scores or to boost their performance on very high-dimensional datasets. \n",
    "\n",
    "\n",
    "### Feature Extraction\n",
    "This involves the construction of new features called __latent features__ (these are features, or topics, that combine a number of other features in your dataset and are therefore representative of those combined features). Constructing these latent features is exactly the goal of principal component analysis (PCA). Independent component analysis (ICA) and Random Projection are other methods of feature extraction.  \n",
    "\n",
    "This is a method that can mean that prevents you from just dropping features directly. \n",
    "\n",
    "\n",
    "Other information: \n",
    "* [Introduction to feature selection](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/)\n",
    "* [Dimensionality Reduction Algorithms](https://elitedatascience.com/dimensionality-reduction-algorithms)\n",
    "* [Introduction to Variable and Feature Selection](http://www.ai.mit.edu/projects/jmlr/papers/volume3/guyon03a/source/old/guyon03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components \n",
    "An advantage of Feature Extraction over Feature Selection is that the latent features can be constructed to incorporate data from multiple features, and thus retain more information present in the various original inputs rather than losing the information by dropping many original inputs. \n",
    "\n",
    "The latent features that are created as a mixture of the existing features are known as principal components; principal components are linear combinations of the original features in a dataset that aim to retain the most information in the original data.  \n",
    "\n",
    "You want to shrink the space that the data lives in, you are change multiple dimensions into a single dimension.   \n",
    "\n",
    "> A principal component is essentially a new feature that is a linear combination of existing features. This may mean that you don't need the original features anymore. They are a latent variable that reduces the number of features to retain the most amount of information in the dataset. \n",
    "\n",
    "### The properties of principal components \n",
    "#### Capture the most variance in the dataset \n",
    "If you select those that capture the most variance in a dataset then you lose the least amount of information. Each subsequent component then captures the largest amount of variance left in the data.  \n",
    "\n",
    "To do this you are looking for a line that reduced the distance of the points to the component across all the points (like regression). The amount fo information lost is the distance between the component created and the original data points (sort of like an error function for other algorithms).\n",
    "\n",
    "\n",
    "#### Created components are orthogonal to one another \n",
    "Orthogonal components are those that have 90 degree angles with one another. \n",
    "\n",
    "Additional components will be orthogonal to one another. Depending on how these components are then used means that we can keep features independent for further analysis.  \n",
    "\n",
    "\n",
    "[Q & A about PCA](https://stats.stackexchange.com/questions/110508/questions-on-pca-when-are-pcs-independent-why-is-pca-sensitive-to-scaling-why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
