{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models and Cluster Validation\n",
    "## Gaussian Mixture Models\n",
    "\n",
    "Gaussian Mixtures Models are soft-clustering, meaning that each point belongs to every single cluster that is created, but they have different levels for each of those clusters; think of this as though each points has a range of clusters that it belongs to, but one of them is likely more likely than the rest, rather than having those hard boundaries like other algorithms such as K Means.  \n",
    "\n",
    "It will group the data points into Gaussian distributions, saying \"These points look as though they form a Gaussian distribution\". On this basis, a point may belong to any of the distirbutions, it is just more likely that they will be in the one where it is nearest the center of that distribution. Note that this involves applying the Gaussian distribution in multiple dimensions, as you are unlikely to be clustering a dataset that contains only a single feature; if this was the case then you would just plot multiple Gaussian distributions over a line of points, creating clusters where the points are most densely populated.  \n",
    "\n",
    "When you combine these features, you create a mixture of Gaussian distrbutions, that result in an overall distribution that isn't Gaussian in its nature. \n",
    "\n",
    "![Gaussian Mixture Model Example in 1 Dimension](./assets/gaussian_mixture_models_1d.png \"Gaussian Mixture Model Example in 1 Dimension\")\n",
    "\n",
    "We can see here that it has clustered this into two clusters, where the orange points represent those that are more likely to come from the leftmost distribution/peak and the purple ones are more likely to come from the rightmost distribution/peak.  \n",
    "\n",
    "When looking at Gaussian Mixture Models in 2 dimensions, you can think of it as concentric circles, with the center point being where the two means meet, and the standard deviations radiating out from there. If the distributions of the two variables aren't equal then they would be circles but will rather stretch out depending on how tight the distributions are for each feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation - Maximisation for Gaussian Mixtures\n",
    "1. Initiliase K Gaussian Distributions\n",
    "2. Soft-cluster data (Expectation step)\n",
    "3. Re-estimate the Gaussians (Maximisation step)\n",
    "4. Evaluate Log-likelihood to check for convergence \n",
    "5. Repeat from 2. until converged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initialising the distributions, you have to give it a starting mean/s and standard deviation/s. You could use first perform K Means, and then use those points as the starting points for your Gaussian distributions, or you can just randomly assign them values to begin with.  \n",
    "\n",
    "The expectation step is using the distribution features to calculate the likelihood of each point belonging to each cluster; what are the __expected__ values for the membership of each cluster to each point. We want to be able to say that each point more likely cam from Gaussian A than Gaussian N for example.  \n",
    "\n",
    "Once we have those membership values for each cluster/distribution then we are able to change the overlying distributions to better 'fit' the data. You go through each point and calculate the weighted averages to find a new center point, or mean. In this way you are giving higher weight to those that you are more certain about in terms of their membership; this is similar to the way in which K Means moves nearer to the points that are assigned to its cluster. The same is done for the standard deviation; the calculation of a weight variance to fine tune the distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood tells us how sure we are that the model that we have created fits the data that we are seeing; the higher the value then the more 'sure' we are. This is how we are going to measure convergence. When this stops increasing (reaches a maximum value) or has plateaued then we stop and say that the model has converged. \n",
    "\n",
    "### Hyperparameters for Gaussian Mixture Models \n",
    "The initialisation points for the clusters can be manual, or using K Means as previously discussed.  \n",
    "\n",
    "The covariance type can be spherical which means that there is a single standard deviation that is applied in all directions, or \"Full\" which allows for the rings of the distribution to be eliptical and find potentially more complex patterns in the data as the shape is working in more than a single dimension.  \n",
    "\n",
    "### Implementation in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you pull the data attribute from the load\n",
    "iris = datasets.load_iris().data\n",
    "gmm = mixture.GaussianMixture(n_components = 3)\n",
    "gmm.fit(iris)\n",
    "labels = gmm.predict(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100,\n",
       "        means_init=None, n_components=3, n_init=1, precisions_init=None,\n",
       "        random_state=None, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "        verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the `covariance_type` that I mentioned before, and the `init_params` which defaults to using the K Means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of Gaussian Mixture Models \n",
    "### Advantages \n",
    "* Soft-clustering (sample membership of multiple clusters)\n",
    "* Cluster shape flexibility through full covariance type\n",
    "\n",
    "### Disadvantages \n",
    "* Sensitive to initialisation values \n",
    "* Possible to converge to local minimum\n",
    "* Slow convergence rate \n",
    "\n",
    "You can use Gaussian Models to look at images, and then work out the most common value for each of the pixels over time as you stream the video; you can then use this to produce an image of the background, with the moving objects of the foreground removed because as the model learns, it will average out those foreground images and produce a picture that is just the background."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
