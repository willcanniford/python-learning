{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "This algorithm intends to place a line between the groups of your data, while maintaining the largest distance between the two groups, thus splitting the groups down the middle.  \n",
    "\n",
    "An SVM works by trying to maximise the distance between the separating line and the points of the data. We create a set of parallel lines that go alongside our central line, known as the margin, and the algorithm is designed to maximise the distance between the line and the margin. \n",
    "\n",
    "It all revolves around minimising the error function, but with this time we have an additional error that is associated with our margins and not just the accuracy of the model and the classifications that it performs. That way we incorporate the margin and thus the placement of the line into the creation of the final model; we don't want points within our margin, and we also would like the margin to be as wide as possible.  \n",
    "\n",
    "Normally you would penalise those misclassified points from the main line, but with SVM you punish those that are near the line as well, marking them as misclassified, so that they have an impact on moving the line through the gradient descent iterations.  \n",
    "\n",
    "Remember that we are adding an error metric that is associated with the size of the margin; we want to create a margin with a large margin, so we inversely punish that by giving a large error value to models that have small margins. This will stop the model generating a non-existent margin to avoid additional classification errors. \n",
    "\n",
    "## Polynomial method\n",
    "\n",
    "Further to this, we add a constant C that is a weight on the classification error that allows us to dictate which is more important to us, classification error or margin error; this will depend on the scenario and whether we can afford to get things wrong when we are looking at our problem.  \n",
    "\n",
    "> Large C means that we classify points very well but have a small margin \n",
    "\n",
    "When the points cannot be separated by a straight line in 2 dimensions, we can think about expanding that to 5 dimensions or higher so that we have more combinations of polynomial functions to work with in order to find the best classifying solution: brings in `x^2` and `xy` and `y^2` which allows for the drawing of hyperboles and circles to try and find a solution that can be applied at the original level of dimensions. This is known as the _kernel trick_. \n",
    "\n",
    "> Using a higher degree polynomial we add more dimensions to the data, find a higher dimensional surface that separates the points, predict it down and we get our curves in the original dimension. \n",
    "\n",
    "The degree of polynomial is a hyperparameter that we can train to find the best possible model. \n",
    "\n",
    "## RBF algorithm \n",
    "This is similar to the kernel (polynomial) trick, whereby you push into higher dimensions and then find a function in that higher degree polynomial that can separate the points. This is done through the generation of 'mountain ranges' using distributions around the different classified points; pushing one class down and the other up produces a higher degree plane with peaks and valleys that can be separated by a cutting line.  \n",
    "\n",
    "Gamma is the main hyperparameter here and decides the steepness of the 'mountains' as it dictates the spread of the distribution curves over the points. A large gamma value means that the spread is small and the model tends to overfit the data by drawing a ring around each point (the mountains are steep and include sometimes only a single point), while a lower value for gamma may underfit the data and mean that there are some miss classifications where a valley for one class doesn't lower it enough if it is near a cluster of points from the other class.  \n",
    "\n",
    "This is probably preferred generally, and is particularly efficient when points from one class are surrounded by another for whatever reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `sklearn` implementation\n",
    "## Prepare our data for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SVC class from the svm library\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generic use example\n",
    "# model = SVC()\n",
    "# model.fit(x_values, y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of using some actual data, we can pull in one of the toy datasets that is included with the `sklearn` library; let's have a look at the breast cancer wisconsin dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data.data)\n",
    "y = np.array(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X) == len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have loaded that in, and have made sure that we have the correct number of elements in both our variables and outcome arrays, we can continue to making our support vector machine model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the accuracy metric to assess model performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import the train test split element to test out of sample performance\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that are good about using the `sklearn.model_selection` are that the `random_state` can be set to increase reproducability, and that you can use the `train_test_split` to make sure that your classes are evenly spread between test and train sets.  \n",
    "\n",
    "If you were to just take the first 80% for training and the last 20% of testing then you couldn't be sure that there wasn't an ordering bias present in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([44, 70]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([168, 287]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "Let's train the main types of models that we have talked about previously, and then compare their predictions using the default hyperparameters for each kernel type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the polynomial model\n",
    "poly_model = SVC(kernel = 'poly').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default selection\n",
    "rbf_model = SVC(kernel = 'rbf').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_pred = poly_model.predict(X_test)\n",
    "print(\"Polynomial kernel accuracy: {}\".format(accuracy_score(y_test, poly_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pred = rbf_model.predict(X_test)\n",
    "print(\"RBF kernel accuracy\".format(accuracy_score(y_test, rbf_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
