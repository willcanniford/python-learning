{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "This algorithm intends to place a line between the groups of your data, while maintaining the largest distance between the two groups, thus splitting the groups down the middle.  \n",
    "\n",
    "An SVM works by trying to maximise the distance between the separating line and the points of the data. We create a set of parallel lines that go alongside our central line, known as the margin, and the algorithm is designed to maximise the distance between the line and the margin. \n",
    "\n",
    "It all revolves around minimising the error function, but with this time we have an additional error that is associated with our margins and not just the accuracy of the model and the classifications that it performs. That way we incorporate the margin and thus the placement of the line into the creation of the final model; we don't want points within our margin, and we also would like the margin to be as wide as possible.  \n",
    "\n",
    "Normally you would penalise those misclassified points from the main line, but with SVM you punish those that are near the line as well, marking them as misclassified, so that they have an impact on moving the line through the gradient descent iterations.  \n",
    "\n",
    "Remember that we are adding an error metric that is associated with the size of the margin; we want to create a margin with a large margin, so we inversely punish that by giving a large error value to models that have small margins. This will stop the model generating a non-existent margin to avoid additional classification errors. \n",
    "\n",
    "Further to this, we add a constant C that is a weight on the classification error that allows us to dictate which is more important to us, classification error or margin error; this will depend on the scenario and whether we can afford to get things wrong when we are looking at our problem.  \n",
    "\n",
    "> Large C means that we classify points very well but have a small margin \n",
    "\n",
    "When the points cannot be separated by a straight line in 2 dimensions, we can think about expanding that to 5 dimensions or higher so that we have more combinations of polynomial functions to work with in order to find the best classifying solution: brings in `x^2` and `xy` and `y^2` which allows for the drawing of hyperboles and circles to try and find a solution that can be applied at the original level of dimensions. This is known as the _kernel trick_. \n",
    "\n",
    "> Using a higher degree polynomial we add more dimensions to the data, find a higher dimensional surface that separates the points, predict it down and we get our curves in the original dimension. \n",
    "\n",
    "The degree of polynomial is a hyperparameter that we can train to find the best possible model. \n",
    "\n",
    "## RBF algorithm \n",
    "This is similar to the kernel trick, whereby you push into higher dimensions and then find a function in that higher degree polynomial that can separate the points. This is done through the generation of 'mountain ranges' using distributions around the different classified points; pushing one class down and the other up produces a higher degree plane with peaks and valleys that can be separated by a cutting line.  \n",
    "\n",
    "Gamma is the main hyperparameter here and decides the steepness of the 'mountains' as it dictates the spread of the distribution curves over the points. A large gamma value means that the spread is small and the model tends to overfit the data by drawing a ring around each point (the mountains are steep and include sometimes only a single point), while a lower value for gamma may underfit the data and mean that there are some miss classifications where a valley for one class doesn't lower it enough if it is near a cluster of points from the other class. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
